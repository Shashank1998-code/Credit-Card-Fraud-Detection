# Credit-Card-Fraud-Detection
Supervised Binary Classification. A comparitive Analysis of Decision Tree and Random Forest Models to identify Fraudulent Card Transactions.



In this machine learning project, we solve the problem of detecting credit card fraud transactions using machine numpy, scikit learn, and few other python libraries. 
We overcome the problem by creating a binary classifier and experimenting with various machine learning techniques to see which fits better.

Credit Card Fraud Dataset

The dataset consists of 31 parameters. Due to confidentiality issues, 28 of the features are the result of the PCA transformation. 
“Time’ and “Amount” are the only aspects that were not modified with PCA.

There are a total of 284,807 transactions with only 492 of them being fraud. So, the label distribution suffers from imbalance issues.

Data : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud

This is actually a binary classification problem as we have to predict only 1 of the 2 class labels. 
We can apply a variety of algorithms for this problem like Random Forest, Decision Tree, Support Vector Machine algorithms, etc.

In this machine learning project, we build Random Forest and Decision Tree classifiers and see which one works best. 
We address the “class imbalance” problem by picking the best-performed model.



The Decision Tree algorithm is a supervised machine learning algorithm used for classification and regression tasks. 
The algorithm’s aim is to build a training model that predicts the value of a target class variable 
by learning simple if-then-else decision rules inferred from the training data.

Random forest (one of the most popular algorithms) is a supervised machine learning algorithm. 
It creates a “forest” out of an ensemble of “decision trees”, which are normally trained using the “bagging” technique. 
The bagging method’s basic principle is that combining different learning models improves the outcome.

To get a more precise and reliable forecast, random forest creates several decision trees and merges them.

CLASS IMBALANCE:

Our dataset suffers a serious problem of class imbalance. The genuine (not fraud) transactions are more than 99% with the credit card fraud transactions constituting 0.17%.

With such a distribution, if we train our model without taking care of the imbalance issues, it predicts the label with higher importance given to genuine transactions (as there is more data about them) and hence obtains more accuracy.

The class imbalance problem can be solved by various techniques. Oversampling is one of them.

Oversample the minority class is one of the approaches to address the imbalanced datasets. The easiest solution entails doubling examples in the minority class, even though these examples contribute no new data to the model.

Instead, new examples may be generated by replicating existing ones. The Synthetic Minority Oversampling Technique, or SMOTE for short, is a method of data augmentation for the minority class.









